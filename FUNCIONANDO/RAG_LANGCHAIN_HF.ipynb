{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFAq5dZuJPyQ96Eh2VKm3/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psygorozco/ML/blob/main/RAG_LANGCHAIN_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XelIUDXp9Cwg"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "u1B39Gcs9KCd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the dataset name and the column containing the content\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "page_content_column = \"context\"  # or any other column you're interested in\n",
        "\n",
        "# Create a loader instance\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
        "\n",
        "# Load the data\n",
        "data = loader.load()\n",
        "\n",
        "# Display the first 15 entries\n",
        "data[:15]"
      ],
      "metadata": {
        "id": "R9NmwjeO9poT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.\n",
        "# It splits text into chunks of 1000 characters each with a 150-character overlap.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "# 'data' holds the text you want to split, split the text into documents using the text splitter.\n",
        "docs = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "opir_1-8-BLo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "tUATnX7v-Fp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the pre-trained model you want to use\n",
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "\n",
        "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
        "model_kwargs = {'device':'cpu'}\n",
        "\n",
        "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,     # Provide the pre-trained model's path\n",
        "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
        ")"
      ],
      "metadata": {
        "id": "Z95-fujJ-GOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a test document.\"\n",
        "query_result = embeddings.embed_query(text)\n",
        "query_result[:5]"
      ],
      "metadata": {
        "id": "-r_3vKz3-UKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "4_skSeVp-YeS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creo que esta linea se puede borrar"
      ],
      "metadata": {
        "id": "UVugNOw_AAks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model name you want to use\n",
        "model_name = \"Intel/dynamic_tinybert\"\n",
        "\n",
        "# Load the tokenizer associated with the specified model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Define a question-answering pipeline using the model and tokenizer\n",
        "question_answerer = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model_name,\n",
        "    tokenizer=tokenizer,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
        "# with additional model-specific arguments (temperature and max_length)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=question_answerer,\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
        ")"
      ],
      "metadata": {
        "id": "bHMwOe1-_Uny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrievers"
      ],
      "metadata": {
        "id": "X7akYYylAWDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever object from the 'db' using the 'as_retriever' method.\n",
        "# This retriever is likely used for retrieving data or documents from the database.\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "GNd179QoAO5P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"What is Cheesemaking?\")\n",
        "print(docs[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L6VZzdKAQh7",
        "outputId": "ab377144-3bf8-44d1-cebd-660665bcc6b6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cut using long, blunt knives and 'blocked' (stacked, cut and turned) by the cheesemaker to promote the release of cheese whey in a process known as 'cheddaring'. During this process the acidity of the curd increases to a desired level. The curd is then milled into ribbon shaped pieces and salt is mixed into it to arrest acid development. The salted green cheese curd is put into cheese moulds lined with cheesecloths and pressed overnight to allow the curd particles to bind together. The pressed blocks of cheese are then removed from the cheese moulds and are either bound with muslin-like cloth, or waxed or vacuum packed in plastic bags to be stored for maturation. Vacuum packing removes oxygen and prevents mould (fungal) growth during maturation, which depending on the wanted final product may be a desirable characteristic or not.\\n\\nMould-ripening\\nMain article: Cheese ripening\\nIn contrast to cheddaring, making cheeses like Camembert requires a more gentle treatment of the curd. It\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval QA Chain\n"
      ],
      "metadata": {
        "id": "WK-S4W1XAexh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Create a question-answering instance (qa) using the RetrievalQA class.\n",
        "# It's configured with a language model (llm), a chain type \"refine,\" the retriever we created, and an option to not return source documents.\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)"
      ],
      "metadata": {
        "id": "TaDuNmyLAfRp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will get the appropriate results, but with a ValueError, where it will ask you to input as a dictionary. While, in your final code, you have given your input as a dictionary."
      ],
      "metadata": {
        "id": "FIvlWKUAZnI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are some unique curtain tie backs that you can make yourself?\"\n",
        "result = qa.run({\"query\": question})\n",
        "print(result[\"result\"])"
      ],
      "metadata": {
        "id": "Q-2yXWSnAjXn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
